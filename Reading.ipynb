{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import os\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Intialement la taille de X_train est (50000, 206) et celle de y_train (50000, 450)\nIntialement la taille de X_test est (2500, 206) et celle de y_test (2500, 450)\nAprès réduction, la taille de X_train est (200, 206) et celle de y_train (200, 450)\nAprès réduction, la taille de X_test est (200, 206) et celle de y_test (200, 450)\n"
     ]
    }
   ],
   "source": [
    "# Kind of data spliting (already split)\n",
    "from problem import get_train_data, get_test_data\n",
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()\n",
    "\n",
    "print(f\"Intialement la taille de X_train est {X_train.shape} et celle de y_train {y_train.shape}\")\n",
    "print(f\"Intialement la taille de X_test est {X_test.shape} et celle de y_test {y_test.shape}\")\n",
    "\n",
    "# Data reducing\n",
    "def decrease_data_size(X, y):\n",
    "    # decrease datasize\n",
    "    X = X.reset_index(drop=True)  # make sure the indices are ordered\n",
    "    # 1. take only first 2 subjects\n",
    "    subjects_used = np.unique(X['subject'])[:2]\n",
    "    X = X.loc[X['subject'].isin(subjects_used)]\n",
    "\n",
    "    # 2. take only n_samples from each of the subjects\n",
    "    n_samples = 100  # use only 100 samples/subject\n",
    "    X = X.groupby('subject').apply(\n",
    "        lambda s: s.sample(n_samples, random_state=42))\n",
    "    X = X.reset_index(level=0, drop=True)  # drop subject index\n",
    "\n",
    "    # get y corresponding to chosen X\n",
    "    y = y[X.index]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = decrease_data_size(X_train, y_train)\n",
    "X_test, y_test = decrease_data_size(X_test, y_test)\n",
    "\n",
    "print(f\"Après réduction, la taille de X_train est {X_train.shape} et celle de y_train {y_train.shape}\")\n",
    "print(f\"Après réduction, la taille de X_test est {X_test.shape} et celle de y_test {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) MODEL KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS = -1\n",
    "\n",
    "def get_estimator():\n",
    "\n",
    "    # K-nearest neighbors\n",
    "    clf = KNeighborsClassifier(n_neighbors=3,algorithm='brute')\n",
    "    kneighbors = MultiOutputClassifier(clf, n_jobs=N_JOBS)\n",
    "\n",
    "    preprocessor = make_column_transformer(('drop', ['subject', 'L_path']), #drop columns with type objects\n",
    "                                           remainder='passthrough')\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('transformer', preprocessor),\n",
    "        ('classifier', kneighbors)\n",
    "    ])\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_estimator()\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_knn = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORMANCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Jaccard error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The jaccard error for KNN is 0.995\n"
     ]
    }
   ],
   "source": [
    "# Jaccard high = predicteur nul\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "score_knn = 1 - jaccard_score(y_test, y_pred_knn, average='samples')\n",
    "print(f\"The jaccard error for KNN is {score_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['lead_field', 'parcel_indices', 'signal_type']\n(204, 4688)\n(4688,)\n(204, 4690)\n(4690,)\n"
     ]
    }
   ],
   "source": [
    "path_subject_1 = X_train[X_train['subject'] == 'subject_1']['L_path'].iloc[0]\n",
    "path_subject_2 = X_train[X_train['subject'] == 'subject_2']['L_path'].iloc[0]\n",
    "\n",
    "lead_field_1 = np.load(path_subject_1)\n",
    "lead_field_2 = np.load(path_subject_2)\n",
    "print(lead_field_1.files)\n",
    "\n",
    "#lead_field['lead_field'] is of shape (n_sensors, n_locations).\n",
    "#lead_field['parcel_indices'] is of shape n_locations and tells us in wchich part of brain is the localisation\n",
    "print(lead_field_1[\"lead_field\"].shape)\n",
    "print(lead_field_1[\"parcel_indices\"].shape)\n",
    "\n",
    "print(lead_field_2[\"lead_field\"].shape)\n",
    "print(lead_field_2[\"parcel_indices\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "def _get_coef(est):\n",
    "    \"\"\"Get coefficients from a fitted regression estimator.\"\"\"\n",
    "    if hasattr(est, 'steps'):\n",
    "        return est.steps[-1][1].coef_\n",
    "    return est.coef_\n",
    "\n",
    "\n",
    "class SparseRegressor(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    '''Provided regression estimator (ie model) solves inverse problem\n",
    "        using data X and lead field L. The estimated coefficients (est_coef\n",
    "        sometimes called z) are then used to predict which parcels are active.\n",
    "\n",
    "        X must be of a specific structure with a column name 'subject' and\n",
    "        'L_path' which gives the path to lead_field files for each subject\n",
    "    '''\n",
    "    def __init__(self, model, n_jobs=1):\n",
    "        self.model = model\n",
    "        self.n_jobs = n_jobs\n",
    "        self.parcel_indices = {}\n",
    "        self.Ls = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.decision_function(X) > 0).astype(int)\n",
    "\n",
    "    def _run_model(self, model, L, X):\n",
    "        norms = np.linalg.norm(L, axis=0)\n",
    "        L = L / norms[None, :]\n",
    "\n",
    "        est_coefs = np.empty((X.shape[0], L.shape[1]))\n",
    "        for idx, idx_used in enumerate(X.index.values):\n",
    "            x = X.iloc[idx].values\n",
    "            model.fit(L, x)\n",
    "            est_coef = np.abs(_get_coef(model))\n",
    "            est_coef /= norms\n",
    "            est_coefs[idx] = est_coef\n",
    "\n",
    "        return est_coefs.T\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        for subject_id in np.unique(X['subject']):\n",
    "            if subject_id not in self.Ls:\n",
    "                # load corresponding L if it's not already in\n",
    "                L_used = X[X['subject'] == subject_id]['L_path'].iloc[0]\n",
    "                lead_field = np.load(L_used)\n",
    "                self.parcel_indices[subject_id] = lead_field['parcel_indices']\n",
    "\n",
    "                # scale L to avoid tiny numbers\n",
    "                self.Ls[subject_id] = 1e8 * lead_field['lead_field']\n",
    "                assert (self.parcel_indices[subject_id].shape[0] ==\n",
    "                        self.Ls[subject_id].shape[1])\n",
    "\n",
    "        n_parcels = np.max([np.max(s) for s in self.parcel_indices.values()])\n",
    "        betas = np.empty((len(X), n_parcels))\n",
    "        for subj_idx in np.unique(X['subject']):\n",
    "            L_used = self.Ls[subj_idx]\n",
    "\n",
    "            X_used = X[X['subject'] == subj_idx]\n",
    "            X_used = X_used.drop(['subject', 'L_path'], axis=1)\n",
    "\n",
    "            est_coef = self._run_model(self.model, L_used, X_used)\n",
    "\n",
    "            beta = pd.DataFrame(\n",
    "                np.abs(est_coef)\n",
    "            ).groupby(self.parcel_indices[subj_idx]).max().transpose()\n",
    "            betas[X['subject'] == subj_idx] = np.array(beta)\n",
    "        return betas"
   ]
  }
 ]
}